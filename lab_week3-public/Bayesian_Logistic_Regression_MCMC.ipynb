{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Statistical Inference -- MCMC for Bayesian Logistic Regression\n",
    "\n",
    "\n",
    "In this notebook, you will learn how to \n",
    "\n",
    "- Implement the MH algorithm,\n",
    "- Use it to compute classification probabilities.\n",
    "- Understand how to run diagnostics on MCMC runs\n",
    "\n",
    "# 1. Model and data\n",
    "\n",
    "In this lab, you’re going to implement the Metropolis-Hasting algorithm described in the lecture for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import warnings\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "import scipy.stats\n",
    "from matplotlib import rc\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "colab = \"google.colab\" in str(get_ipython())\n",
    "preamble = r\"\"\"\\renewcommand{\\familydefault}{\\sfdefault}\\usepackage{sansmath}\n",
    "\\usepackage{FiraSans}\\sansmath\\usepackage{amsmath}\"\"\"\n",
    "\n",
    "rc(\"font\", **{\"family\": \"sans-serif\", \"sans-serif\": \"DejaVu Sans\"})\n",
    "rc(\"text\", **{\"usetex\": not colab, \"latex.preamble\": preamble})\n",
    "rc(\"figure\", **{\"dpi\": 200})\n",
    "rc(\n",
    "    \"axes\",\n",
    "    **{\"spines.right\": False, \"spines.top\": False, \"xmargin\": 0.0, \"ymargin\": 0.05}\n",
    ")\n",
    "\n",
    "\n",
    "def plot_data(X, y, ax):\n",
    "    mask = y == 1\n",
    "    config = dict(edgecolor=\"black\", linewidth=1, zorder=10)\n",
    "    ax.scatter(*X[mask].T, label=\"Class 1\", facecolor=\"tab:blue\", **config)\n",
    "    ax.scatter(*X[~mask].T, label=\"Class 0\", facecolor=\"tab:orange\", **config)\n",
    "\n",
    "\n",
    "def get_grid(xlim=(-3, 3), ylim=None, N=100):\n",
    "    if ylim is None:\n",
    "        ylim = xlim\n",
    "    x_grid = np.linspace(*xlim, N)\n",
    "    y_grid = np.linspace(*ylim, N)\n",
    "    xx, yy = np.meshgrid(x_grid, y_grid)\n",
    "    X_plot = np.vstack((xx.flatten(), yy.flatten())).T\n",
    "    return xx, yy, X_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "Load and plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"binaryclass2.csv\", delimiter=\",\")\n",
    "X = data[..., :-1]\n",
    "y = data[..., -1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 3])\n",
    "# @@ COMPLETE @@ #\n",
    "ax.set_title(\"Binary classification\")\n",
    "ax.legend()\n",
    "ax.set_xlim(-6, 6), ax.set_ylim(-6, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic regression (binary), we use the logistic (or sigmoid) function defined like $h(z) = (1+\\exp(-z))^{-1}$.\n",
    "\n",
    "**Exercise:**\n",
    "Implement the logistic function and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return # @@ COMPLETE @@ #\n",
    "\n",
    "\n",
    "z = np.linspace(-10, 10, 100)\n",
    "fig, ax = plt.subplots(figsize=[4, 2])\n",
    "ax.plot(z, logistic(z))\n",
    "ax.set_ylim(0, 1.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood that we will use is the *Bernoulli likelihood*. Its (log)density is defined as follows\n",
    "\n",
    "$$\\log p({y}|{p}) = \\log({p}) \\quad\\text{if } y = 0$$\n",
    "$$\\log p({y}|{p}) = \\log(1 - {p}) \\quad\\text{if } y= 1$$\n",
    "\n",
    "where ${y}$ is the target class [0, 1] and ${p}$ is the predictive probability (i.e. the output of the logistic regression).\n",
    "\n",
    "**Exercise:**\n",
    "Complete the following function to compute the Bernoulli loglikelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_density(y, p):\n",
    "    return # @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Complete also the code for computing the Gaussian (log)density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_density(x, mean=0, var=1):\n",
    "    return # @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Random Walk with Metropolis-Hastings correction\n",
    "\n",
    "Check the lecture notes for the full analysis of the sampler. Below you will find just a summary:\n",
    "\n",
    "1. Produces a sequence of samples – $\\boldsymbol{w}_1, \\boldsymbol{w}_2, \\dots, \\boldsymbol{w}_s, \\dots$\n",
    "- Imagine we’ve just produced $\\boldsymbol{w}_{s-1}$\n",
    "- MH firsts proposes a possible $\\boldsymbol{w}_s$ (call it $\\boldsymbol{\\tilde w}_s$) based on $\\boldsymbol{w}_{s-1}$.\n",
    "- MH then decides whether or not to accept wfs\n",
    "    - If accepted, $\\boldsymbol{w}_s \\leftarrow \\boldsymbol{\\tilde w}_s$\n",
    "    - If not, $\\boldsymbol{w}_s \\leftarrow \\boldsymbol{w}_{s-1}$\n",
    "\n",
    "We need to treat $\\boldsymbol{\\tilde w}_s$ as a random variable conditioned on $\\boldsymbol{w}_{s-1}$. We can choose whatever we like but a simple solution is to use a Gaussian centered on  $\\boldsymbol{w}_{s-1}$ with some covariance $\\boldsymbol{\\Sigma}_p$. \n",
    "\n",
    "Regarding the acceptance, we need to compute the acceptance ratio. Check the lecture notes for the full derivation.\n",
    "The first thing that we need to compute is the un-normalized logposterior (i.e. the sum of loglikelihood and prior):\n",
    "\n",
    "$$\n",
    "\\log p(\\boldsymbol{w}|\\boldsymbol{X},\\boldsymbol{y}) \\propto \\log p(\\boldsymbol{y}|\\boldsymbol{w}, \\boldsymbol{X}) + \\log p(\\boldsymbol{w}) := g(\\boldsymbol{w}; \\boldsymbol{X}, \\boldsymbol{y})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Exercise:**\n",
    "Complete the function below with the code to compute the unnormalized logdensity $g(\\boldsymbol{w}; \\boldsymbol{X}, \\boldsymbol{y})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logposterior(w, X, y):\n",
    "    # @@ COMPLETE @@ #\n",
    "    return # @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Exercise:**\n",
    "Now you can move to the actual random walk with MH. Complete the `rw_mh_step()` function following the flowchart in the slides. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rw_mh_step(w_prev, step_size):\n",
    "    # @@ COMPLETE @@ #\n",
    "    return w_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you have a simple function to run the sampling for a given number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sampling(w, n_samples, step_size=1):\n",
    "    w = np.atleast_1d(w)\n",
    "    samples = np.empty((n_samples, *w.shape))\n",
    "    for i in range(n_samples):\n",
    "        w = rw_mh_step(w, step_size)\n",
    "        samples[i] = w\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Run the sampler for 10000 steps (you can fix the step size for the proposal to 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init = [0, 0]\n",
    "samples = # @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Plot the samples and their distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_posterior(ax):\n",
    "    xx, yy, X_plot = get_grid(xlim=(-1.5, 5.5), N=250)\n",
    "    vals = np.zeros(len(X_plot))\n",
    "    for i, w in enumerate(X_plot):\n",
    "        vals[i] = np.exp(logposterior(w, X, y))\n",
    "    levels = np.linspace(1e-5, max(vals), 10)\n",
    "    patch = ax.contourf(\n",
    "        xx, yy, vals.reshape(*xx.shape), cmap=\"cividis\", alpha=0.6, levels=levels\n",
    "    )\n",
    "    ax.contour(xx, yy, vals.reshape(*xx.shape), cmap=\"cividis\", levels=patch.levels[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[5, 4])\n",
    "ax.scatter(\n",
    "    *samples.T, s=4, edgecolor=\"black\", linewidth=0.1, zorder=10, facecolor=\"white\"\n",
    ")\n",
    "plot_posterior(ax)\n",
    "ax.set_xlim(-0.5, 3.5), ax.set_ylim(-0.5, 3.5)\n",
    "ax.set_xlabel(r\"$\\mathbf{w}_0$\")\n",
    "ax.set_ylabel(r\"$\\mathbf{w}_1$\")\n",
    "ax.set_title(r\"Samples from $p(\\mathbf{w}|\\mathbf{X},\\mathbf{y}) $\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can make predictions. \n",
    "Remember that our motivation for being Bayesian was to be able to average predictions at $\\boldsymbol{x}_\\mathrm{new}$, for all possible $\\boldsymbol{w}$.\n",
    "This is possible by computing the following expectation:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{E}_{p(\\boldsymbol{w}|\\boldsymbol{X}, \\boldsymbol{y}, \\sigma_\\mathrm{n})}h(\\boldsymbol{w}^\\top\\boldsymbol{x}_\\mathrm{new}) = \\int h(\\boldsymbol{w}^\\top\\boldsymbol{x}_\\mathrm{new}) p(\\boldsymbol{w}|\\boldsymbol{X}, \\boldsymbol{y}) \\mathrm{d}\\boldsymbol{w}\n",
    "$$\n",
    "\n",
    "**Exercise:**\n",
    "Complete the next function to compute this expectation. And compute the probability $P (y_\\mathrm{new} = 1 | \\boldsymbol{x}_\\mathrm{new}, \\boldsymbol{X}, \\boldsymbol{y})$ when $\\boldsymbol{x}_\\mathrm{new} = [2,-4]^\\top$ . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Xt, samples):\n",
    "    # @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict([2, -4], samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Now predict on a grid of points and plot the predictive probabilities (use the two helper function below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(xx, yy, P, ax):\n",
    "    P = P.reshape(*xx.shape)\n",
    "    levels = [0, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 1]\n",
    "    cs = ax.contour(xx, yy, P, levels, colors=\"k\", linewidths=1.8, zorder=100)\n",
    "    ax.clabel(cs, inline=1, fontsize=10)\n",
    "    cs = ax.contourf(xx, yy, P, levels, cmap=\"Purples_r\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy, X_grid = get_grid((-7, 7), N=50)\n",
    "ps = predict(X_grid, samples)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 4])\n",
    "plot_decision_boundary(xx, yy, ps, ax=ax)\n",
    "plot_data(X, y, ax=ax)\n",
    "\n",
    "ax.set_xlabel(r\"$\\mathbf{x}_0$\")\n",
    "ax.set_ylabel(r\"$\\mathbf{x}_1$\")\n",
    "ax.set_title(\"Predictive density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Assessing convergence of MCMC\n",
    "\n",
    "Now that the MCMC sampler is done and working, we can move to some analysis.\n",
    "If your algorithm is designed properly, the Markov chain will converge to the target distribution after *infinite* iterations.\n",
    "We need to decide when is it wise to make inferences based on a finite Markov chain.\n",
    "Assessing the convergence of your MCMC is essential if you want to:\n",
    "\n",
    "- Base your conclusions on posterior distributions\n",
    "- Report accurate parameter estimates & uncertainties\n",
    "- Avoid fooling yourself\n",
    "- Avoid devoting resources to follow-up an “inference” that isn’t supported by data\n",
    "- Avoid writing an erratum to your homework\n",
    "\n",
    "## 3.1 Burn-in\n",
    "\n",
    "**Exercise:**\n",
    "Sometimes choosing the initial point to start the MCMC is not easy. Choose some very wrong poins (say $[-4, -4]^\\top$) and run the sampler for 10000 (set the step size to 0.1).\n",
    "Plot the trajectory of the first 100s steps with a different color. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init = [-4, -4]\n",
    "samples = # @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[5, 4])\n",
    "# @@ COMPLETE @@ #\n",
    "\n",
    "ax.set_xlabel(r\"$\\mathbf{w}_0$\")\n",
    "ax.set_ylabel(r\"$\\mathbf{w}_1$\")\n",
    "ax.set_title(r\"Samples from $p(\\mathbf{w}|\\mathbf{X},\\mathbf{y}) $\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This effect can be mitigated by using burn-in.\n",
    "Burn-in is intended to give the Markov Chain time to reach its equilibrium distribution, particularly if it has started from a lousy starting point. To \"burn in\" a chain, you just discard the first $n$ samples before you start collecting points.\n",
    "\n",
    "The idea is that a \"bad\" starting point may over-sample regions that are actually very low probability under the equilibrium distribution before it settles into the equilibrium distribution. If you throw those points away, then the points which should be unlikely will be suitably rare.\n",
    "\n",
    "It clear that the burn-in is more of a hack/artform than a principled technique. In theory, you could just sample for a really long time or find some way to choose a decent starting point instead. \n",
    "\n",
    "## 3.2 Trace plots\n",
    "\n",
    "The trace plot shows the sampled values of a parameter over time (iterations). This plot helps you to judge how quickly the MCMC procedure converges in distribution—that is, how quickly it forgets its starting values.\n",
    "\n",
    "**Exercise:**\n",
    "For the two parameters of $\\boldsymbol{w}$, plot their trace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(2, 1, figsize=[10, 4], sharex=True)\n",
    "\n",
    "# @@ COMPLETE @@ #\n",
    "\n",
    "ax0.set_ylabel(r\"$\\boldsymbol{w}_0$\")\n",
    "ax1.set_ylabel(r\"$\\boldsymbol{w}_1$\")\n",
    "ax0.set_title(\"Trace plot\")\n",
    "ax1.set_xlabel(r\"Sample index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Change the step size, sample and plot the trace. Try 10, 1, 0.1 and 0.01. What behavior do you see? Comment the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Multiple chains and a more sophisticated diagnostics\n",
    "\n",
    "In an attempt to assuage concerns of poor convergence, we typically run multiple independent chains to see if the\n",
    "obtained distribution is similar across chains. We can also visually inspect the sample paths of the chains via trace plots\n",
    "as well as study summary statistics such as the empirical autocorrelation function.\n",
    "\n",
    "**Exercise:** \n",
    "Complete the following function to run multiple chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiple_chains(w_init, n_chains, n_samples, step_size=0.5):\n",
    "    w_init = np.atleast_1d(w_init)\n",
    "    samples = np.zeros((n_chains, n_samples, *w_init.shape))\n",
    "    # @@ COMPLETE @@ #\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Sample now from multiple independent chains and plot the traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = # @@ COMPLETE @@ #\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=[10, 4], sharex=True)\n",
    "# @@ COMPLETE @@ #\n",
    "axs[0].set_ylabel(r\"$\\boldsymbol{w}_0$\")\n",
    "axs[1].set_ylabel(r\"$\\boldsymbol{w}_1$\")\n",
    "axs[0].set_title(\"Trace plot with multiple chains\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Decrease the step size, run 4 chains and plot the traces. What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = # @@ COMPLETE @@ #\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=[10, 4], sharex=True)\n",
    "# @@ COMPLETE @@ #\n",
    "axs[0].set_ylabel(r\"$\\boldsymbol{w}_0$\")\n",
    "axs[1].set_ylabel(r\"$\\boldsymbol{w}_1$\")\n",
    "axs[0].set_title(\"Trace plot with multiple chains\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the various convergence diagnostics, probably the most widely used is the **potential scale reduction factor** $\\hat R$.\n",
    "It is recommended as the primary convergence diagnostic in widely applied software\n",
    "packages for MCMC sampling such as Stan (Carpenter et al., 2017), JAGS (Plummer, 2003), WinBUGS (Lunn et al.,\n",
    "2000), OpenBUGS (Lunn et al., 2009), PyMC3 (Salvatier et al., 2016), and NIMBLE (de Valpine et al., 2017), which\n",
    "together are estimated to have hundreds of thousand of users. \n",
    "$\\hat R$ is computed for each scalar quantity of interest, as the standard deviation of that quantity from all the chains included together, divided by the root mean square of the separate within-chain standard deviations. The idea is that if a set of simulations have not mixed well, the variance\n",
    "of all the chains mixed together should be higher than the variance of individual chains\n",
    "\n",
    "\n",
    "At convergence, the chains will have mixed, so that the distribution of the simulations\n",
    "between and within chains will be identical, and the ratio $\\hat R$ should equal 1. If $\\hat R$\n",
    "is greater than 1, this implies that the chains have not fully mixed and that further simulation might increase the precision of inferences. In practice we typically go until $\\hat R$ is less than 1.1/1.05 for all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rhat(samples):\n",
    "    \"\"\"Compute the rhat statistics from samples. Samples needs to be a tensor\n",
    "    with dimensions [num_of_chain, num_of_samples, num_of_variables].\"\"\"\n",
    "\n",
    "    def _rhat_base(ary):\n",
    "        \"\"\"Compute the rhat for a 2d array.\"\"\"\n",
    "        _, num_samples = ary.shape\n",
    "\n",
    "        # Calculate chain mean\n",
    "        chain_mean = np.mean(ary, axis=1)\n",
    "        # Calculate chain variance\n",
    "        chain_var = np.var(ary, axis=1, ddof=1)\n",
    "        # Calculate between-chain variance\n",
    "        between_chain_variance = num_samples * np.var(chain_mean, axis=None, ddof=1)\n",
    "        # Calculate within-chain variance\n",
    "        within_chain_variance = np.mean(chain_var)\n",
    "        # Estimate of marginal posterior variance\n",
    "        rhat_value = np.sqrt(\n",
    "            (between_chain_variance / within_chain_variance + num_samples - 1)\n",
    "            / (num_samples)\n",
    "        )\n",
    "        return rhat_value\n",
    "\n",
    "    def _split_chains(ary):\n",
    "        \"\"\"Split and stack chains.\"\"\"\n",
    "        _, n_draw = ary.shape\n",
    "        half = n_draw // 2\n",
    "        return np.vstack((ary[:half], ary[-half:]))\n",
    "\n",
    "    samples = np.atleast_3d(samples)\n",
    "    return np.asarray(\n",
    "        [_rhat_base(_split_chains(samples[..., i])) for i in range(samples.shape[-1])]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Using the function provided above, compute the $\\hat R$-statistics for $\\boldsymbol{w}$ with the following parameters:\n",
    "\n",
    "- num_of_chain = 4\n",
    "- num_of_samples = [100, 1000, 10000]\n",
    "- step size = [1, 0.1, 0.01]\n",
    "\n",
    "**Question:**\n",
    "Comment the results. For which configurations $\\hat R$-statistics suggests convergence has not been achieved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "The $\\hat R$-statistics can also be plotted as a function of iteration. Complete the next code cell to visualize its behaviour.\n",
    "Try also with step size = [0.5, 0.1, 0.01]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.logspace(1, 5, 10).astype(int)\n",
    "rhats = np.vstack([compute_rhat(samples[..., :i, :]) for i in steps])\n",
    "fig, ax = plt.subplots(figsize=[8, 3])\n",
    "ax.plot(steps, rhats, \"o-\")\n",
    "ax.fill_between(steps, 0.9, 1.05, color=\"xkcd:leaf green\", alpha=0.5, lw=0)\n",
    "ax.fill_between(steps, 1.05, 1.15, color=\"xkcd:pumpkin orange\", alpha=0.5, lw=0)\n",
    "ax.set_title(r\"$\\hat R$ vs iterations\")\n",
    "\n",
    "# ax.set_ylim(0.9, 2)\n",
    "ax.legend([r\"$\\boldsymbol{w}_0$\", r\"$\\boldsymbol{w}_1$\"])\n",
    "ax.semilogx()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
