{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Advanced Statistical Inference -- Gaussian Process for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Today, we will play with Gaussian processes. By the end of this lab, you will be able to \n",
    " \n",
    "- To sample from a Gaussian process prior distribution.\n",
    "- To implement Gaussian process inference for regression.\n",
    "- To use the above to observe samples from a Gaussian process posterior distribution.\n",
    "- To evaluate how different hyperparameter settings impact model quality.\n",
    "- To investigate different kernel functions and parameter optimisation strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian processes (henceforth GPs) achieve greater flexibility over parametric models by imposing a preference bias as opposed to restrictive constraints. Although the parameterisation of GPs allows one to access a certain (infinite) set of functions, preference can be expressed using a prior over functions. This allows greater freedom in representing data dependencies, thus enabling the construction of better-suited models. In this lab, we shall cover the basic concepts of GP regression. For the sake of clarity, we shall focus on univariate data, which allows for better visualisation of the GP model. Nonetheless, the code implemented within this lab can be very easily extended to handle\n",
    "multi-dimensional inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "For this notebook, we will use a slightly different library (jax). For what we are concerned, Jax is just numpy with automatic differentiation: it implements all numpy functions and also their derivatives. For what we need, the change in the code is as small as replacing `np.something` with `jnp.something`. That's it! Nothing more!\n",
    "Later in the notebook, we will see how this small change in the code allows to compute gradients in an embarrassingly simple way. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import warnings\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import rc\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "colab = \"google.colab\" in str(get_ipython())\n",
    "preamble = r\"\"\"\\renewcommand{\\familydefault}{\\sfdefault} \\usepackage{FiraSans}\n",
    "            \\usepackage{sansmath} \\sansmath  \\usepackage{amsmath}\"\"\"\n",
    "rc(\"font\", **{\"family\": \"sans-serif\", \"sans-serif\": \"DejaVu Sans\"})\n",
    "rc(\"text\", **{\"usetex\": not colab, \"latex.preamble\": preamble})\n",
    "rc(\"figure\", **{\"dpi\": 200})\n",
    "rc(\n",
    "    \"axes\",\n",
    "    **{\"spines.right\": False, \"spines.top\": False, \"xmargin\": 0.0, \"ymargin\": 0.05}\n",
    ")\n",
    "\n",
    "\n",
    "def plot_data(X, y, ax):\n",
    "    config = dict(edgecolor=\"black\", linewidth=1, facecolor=\"tab:blue\")\n",
    "    ax.scatter(X, y, label=\"Data points\", zorder=10, **config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "For this notebook, we shall consider a one-dimensional regression problem.\n",
    "\n",
    "**Exercise:**\n",
    "Run the next cell to prepare and plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3424121)\n",
    "\n",
    "\n",
    "def make_dataset(a=-20, b=80, N=64, M=200, gap_ratio=0.4, ampl=1.6, leng=8, sn2=0.01):\n",
    "    def make_random_gap(X, gap_ratio=0.2):\n",
    "        a, b = X.min(), X.max()\n",
    "        gap_a = a + np.random.rand() * (b - a) * (1 - gap_ratio)\n",
    "        gap_b = gap_a + (b - a) * gap_ratio\n",
    "        idx = np.logical_and(gap_a < X, X < gap_b)\n",
    "        X[idx] = a + np.random.rand(idx.sum()) * (gap_a - a)\n",
    "        return X\n",
    "\n",
    "    def sample_random_function(X, ampl=1, leng=1, sn2=0.1):\n",
    "        n, x = X.shape[0], X / leng\n",
    "        sum_xx = np.sum(x * x, 1).reshape(-1, 1).repeat(n, 1)\n",
    "        D = sum_xx + sum_xx.transpose() - 2 * np.matmul(x, x.transpose())\n",
    "        C = ampl**2 * np.exp(-0.5 * D) + np.eye(n) * sn2\n",
    "        return np.random.multivariate_normal(np.zeros(n), C)\n",
    "\n",
    "    X = np.random.rand(N, 1) * (b - a) + a\n",
    "    X = make_random_gap(X, gap_ratio=0.4)\n",
    "    ind = np.argsort(X[..., 0])\n",
    "    y = sample_random_function(X, ampl=ampl, leng=leng, sn2=sn2)\n",
    "    Xt = np.linspace(a - 5, b + 5, M).reshape(-1, 1)\n",
    "    return X[ind], y[ind], Xt\n",
    "\n",
    "\n",
    "X, y, Xt = make_dataset()\n",
    "fig, ax = plt.subplots(figsize=[5, 3])\n",
    "plot_data(X, y, ax)\n",
    "ax.set_xlim(Xt.min(), Xt.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sampling from the GP Prior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that since GPs are non-parametric; we define a prior distribution over functions (models),\n",
    "specified as a multivariate Gaussian distribution $p(\\boldsymbol f) = \\mathcal{N} (\\boldsymbol\\mu, \\boldsymbol\\Sigma)$.\n",
    "\n",
    "Without loss of generality, we shall assume a zero-mean GP prior, i.e. $\\boldsymbol\\mu = \\boldsymbol 0$. The covariance\n",
    "matrix of the distribution, $\\boldsymbol\\Sigma$, may then be computed by evaluating the covariance between the\n",
    "input points. For this tutorial, we shall consider the widely used squared-exponential (RBF)\n",
    "covariance (also referred to as the kernel function), which is defined between two points as: \n",
    "\n",
    "$$\\kappa(\\boldsymbol x, \\boldsymbol x') = \\sigma_f^2 \\exp \\Big( -\\dfrac {\\|\\boldsymbol x-\\boldsymbol x'\\|^2}{2l^2} \\Big). $$\n",
    "\n",
    "This kernel is parameterised by a lengthscale parameter $l$, and variance $\\sigma_{f}^2$ . Given that the true\n",
    "function may be assumed to be corrupted with noise, we can also add a noise parameter, $\\sigma_\\mathrm{n}^2$ , to\n",
    "the diagonal entries of the resulting kernel matrix, $\\boldsymbol K_y$, such that\n",
    "\n",
    "$$\\boldsymbol K_y = \\boldsymbol K + \\sigma_{n}^2\\boldsymbol I.$$\n",
    "\n",
    "\n",
    "**Exercise:**\n",
    "Complete the `rbf_kernel()` function for computing the RBF kernel $\\boldsymbol K$ between two sets of input points. For a bit of flexibility\n",
    "the kernel parameters are saved in a dictionary and passed as first argument to the function.\n",
    "Hint: The `cdist` function can be used for evaluating the pairwise squared Euclidean distance between two sets of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdist(A, B):\n",
    "    \"\"\"\n",
    "    Compute all pairwise distances between vectors in A and B.\n",
    "    \"\"\"\n",
    "    M = A.shape[0]\n",
    "    N = B.shape[0]\n",
    "    A_dots = (A * A).sum(axis=1).reshape((M, 1)) * jnp.ones(shape=(1, N))\n",
    "    B_dots = (B * B).sum(axis=1) * jnp.ones(shape=(M, 1))\n",
    "    D_squared = A_dots + B_dots - 2 * A.dot(B.T)\n",
    "    return D_squared\n",
    "\n",
    "\n",
    "def rbf_kernel(params, X1, X2):\n",
    "    lengthscale = params[\"lengthscale\"]\n",
    "    variance = params[\"variance\"]\n",
    "     # @@ COMPLETE @@ #\n",
    "    return # @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Compute the kernel matrix $\\kappa(\\boldsymbol{X}, \\boldsymbol{X})$. Plot it using `plt.imshow()`. Start with $\\sigma^2_f$ and $l$ both equal to one. Then try to change this two parameter. What do you see? Comment the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"lengthscale\": 1.0, \"variance\": 1.0}\n",
    "\n",
    "K =  # @@ COMPLETE @@ #\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bar = ax.imshow(K, extent=[X.min(), X.max(), X.min(), X.max()], cmap=\"cividis\")\n",
    "fig.colorbar(bar)\n",
    "ax.set_title(r\"Covariance matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to sample from the GP prior. Remember if $f$ follows a GP, ie $f \\sim \\mathcal{GP}(\\mu(\\cdot), \\kappa(\\cdot,\\cdot))$, then the function $f$ computed at points $X$ will have this prior $p(\\boldsymbol{f}) = \\mathcal{N}(\\boldsymbol\\mu, \\boldsymbol{\\Sigma})$, where $\\boldsymbol\\mu = \\mu(\\boldsymbol X)$ and $\\boldsymbol\\Sigma = \\kappa(\\boldsymbol X, \\boldsymbol X)$. \n",
    "\n",
    "Below you will find a couple of functions to plot a GP and some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp(x, mean, cov, palette=\"Greens\", **kwargs):\n",
    "    ax = kwargs.pop(\"ax\", plt.gca())\n",
    "    cmap = plt.get_cmap(palette)\n",
    "    ci = [1, 2, 3]\n",
    "    colors = (ci - np.min(ci)) / (np.max(ci) - np.min(ci) + 3) + 0.1\n",
    "    x = x.flatten()\n",
    "    ax.plot(x, mean, color=cmap(0.9), lw=4)\n",
    "    for i, c in enumerate(ci[::-1]):\n",
    "        up = mean + c * np.sqrt(np.diag(cov))\n",
    "        lo = mean - c * np.sqrt(np.diag(cov))\n",
    "        color = cmap(colors[i])\n",
    "        ax.fill_between(x, up, lo, color=color, alpha=0.95, **kwargs)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_samples(x, samples, palette=\"Greens\", **kwargs):\n",
    "    ax = kwargs.pop(\"ax\", plt.gca())\n",
    "    N = kwargs.pop(\"N\", 20)\n",
    "    cmap = plt.get_cmap(palette)\n",
    "    idx = np.random.randint(0, samples.shape[0], N)\n",
    "    ax.plot(x, samples[idx].T, color=cmap(0.5), lw=1, alpha=0.75, **kwargs)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Assuming a zero-mean prior, and using the RBF kernel constructed before, complete the following function to sample from the prior (use `np.random.multivariate_normal()` function).\n",
    "For the time being, you can initialise the kernel parameters as follows:\n",
    "\n",
    "- lengthscale = 15.0\n",
    "- variance = 2.0\n",
    "\n",
    "Note: make sure these parameters are floats (not integers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_f_prior(params, kernel_fn, Xt, N=20):\n",
    "    mean =  # @@ COMPLETE @@ #\n",
    "    cov =  # @@ COMPLETE @@ #\n",
    "    samples =  # @@ COMPLETE @@ #\n",
    "    return samples, mean, cov\n",
    "\n",
    "\n",
    "params = {\"lengthscale\": 15.0, \"variance\": 2.0}\n",
    "\n",
    "samples, mean, cov = sample_f_prior(params, rbf_kernel, Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=[10, 3], sharey=True)\n",
    "plot_samples(Xt, samples, ax=ax0)\n",
    "plot_gp(Xt, mean, cov, ax=ax1)\n",
    "plot_data(X, y, ax1)\n",
    "ax1.legend()\n",
    "fig.suptitle(r\"Gaussian process prior\", y=1.05, fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Try to change the kernel hyperparameters and plot few samples from their corresponding GP prior. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Inference with GP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the prior represents our prior beliefs before observing the function values of any data points. Suppose we can now observe 3 points at random from the input data; we would expect that with this additional knowledge, the functions drawn from the updated GP distribution would be constrained to pass through these points (or at least close if corrupted with noise). The combination of the prior and the likelihood of the observed data leads to the posterior distribution over functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the lecture, you have derived the posterior distribution of the GP given data $\\{\\mathbf{X},\\mathbf{y}\\}$ (check the lecture note if you don't remeber).\n",
    "\n",
    "**Note**: As we have encountered in previous labs, matrix inversions can be both numerically troublesome and slow to compute. \n",
    "In this lab, we shall avoid computing matrix inversions directly by instead considering Cholesky decompositions for solving linear systems. \n",
    "You are encouraged to read more about Cholesky decompositions for GPs by consulting Appendix A.4 of [Gaussian Processes for Machine Learning (Rasmussen and Williams, 2005)](http://www.gaussianprocess.org/gpml/) - available online!\n",
    "The complete pseudo-code for the posterior inference procedure is provided in Algorithm 2.1 from Chapter 2 of this same book.\n",
    "Unfortunately, that Algorithm explains how to compute the posterior one test point at a time. You could loop through all test points but this is not very efficient. Instead you can compute the posterior for all points in one shot, by remembering that\n",
    "\n",
    "\n",
    "$$p(\\boldsymbol{f_\\star}\\,|\\,\\boldsymbol{y}) = \\mathcal{N}(\\boldsymbol{f}_{\\text{mean}}, \\boldsymbol{f}_{\\text{cov}}) $$\n",
    "\n",
    "$$\\boldsymbol{f}_{\\text{mean}} =  \\kappa(\\boldsymbol{X}_\\star, \\boldsymbol{X})\\left[\\kappa(\\boldsymbol{X}, \\boldsymbol{X}) + \\sigma_n^2 I\\right]^{-1}\\boldsymbol{y}$$\n",
    "\n",
    "$$\\boldsymbol{f}_{\\text{cov}} =  \\kappa(\\boldsymbol{X}_\\star, \\mathbf{X}_\\star) - \\kappa(\\boldsymbol{X}_\\star, \\boldsymbol{X})\\left[\\kappa(\\boldsymbol{X}, \\boldsymbol{X}) + \\sigma_{n}^2 I\\right]^{-1}\\kappa(\\boldsymbol{X}_\\star, \\boldsymbol{X})^\\top$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "Complete the following function to compute the GP posterior GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_f_posterior(params, kernel_fn, X, y, Xt, sn2=0.01):\n",
    "     # @@ COMPLETE @@ #\n",
    "    fmean =  # @@ COMPLETE @@ #\n",
    "    fcov =  # @@ COMPLETE @@ #\n",
    "    return fmean, fcov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Assign 3 points at random from $\\boldsymbol{X}$ (and their corresponding function values) to `Xtr` and `ytr`\n",
    "respectively. For now we shall assume that all other $\\boldsymbol{X}$ values are unobserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(14)\n",
    "idx = np.random.permutation(len(X))[:5]\n",
    "\n",
    "Xtr =  # @@ COMPLETE @@ #\n",
    "ytr =  # @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "Run the previously completed function to compute the GP posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Sample few times from the posterior and, using the helper functions defined above, plot the samples and their distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=[10, 3], sharey=True)\n",
    "\n",
    "plot_samples(Xt, samples, palette=\"Oranges\", ax=ax0)\n",
    "plot_data(Xtr, ytr, ax0)\n",
    "plot_gp(Xt, f_mean, f_cov, ax=ax1, palette=\"Oranges\")\n",
    "plot_data(Xtr, ytr, ax1)\n",
    "ax1.legend()\n",
    "fig.suptitle(f\"Gaussian process posterior ($N={len(Xtr)}$)\", y=1.05, fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Try to add more samples. Also plot the posterior with all the data available. What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Try to change the kernel parameters (lengthscale and variance). Plot the posterior for 3/4 combinations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "As a measure of model quality, you should also compute the (log) marginal likelihood of the model.\n",
    "For this, complete the function below to compute the log marginal likelihood (disregard the decorator above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnums=(1))\n",
    "def marginal_likelihood(params, kernel_fn, X, y, sn2=0.01):\n",
    "     # @@ COMPLETE @@ #\n",
    "    return  # @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Compute the marginal likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Run a grid search over a range of parameter values in order to determine which configuration yields the best result (based on the marginal). For this exercise, use the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengthscales = np.linspace(1, 20, 51)\n",
    "variances = np.linspace(1, 20, 50)\n",
    "marginal_ll = np.empty((len(lengthscales), len(variances)))\n",
    "for i, lengthscale in enumerate(lengthscales):\n",
    "    for j, variance in enumerate(variances):\n",
    "        _params = {\"lengthscale\": lengthscale, \"variance\": variance}\n",
    "        marginal_ll[i, j] =  # @@ COMPLETE @@ #\n",
    "\n",
    "best = np.unravel_index(marginal_ll.argmax(), marginal_ll.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "cset = ax.contourf(lengthscales, variances, marginal_ll.T, cmap=\"cividis\", levels=30)\n",
    "fig.colorbar(cset)\n",
    "cset = ax.contour(\n",
    "    lengthscales,\n",
    "    variances,\n",
    "    marginal_ll.T,\n",
    "    colors=\"k\",\n",
    "    linestyles=\"-\",\n",
    "    alpha=0.4,\n",
    "    levels=30,\n",
    ")\n",
    "ax.set_ylabel(\"Variance\")\n",
    "ax.set_xlabel(\"Lengthscale\")\n",
    "\n",
    "ax.plot(\n",
    "    lengthscales[best[0]],\n",
    "    variances[best[1]],\n",
    "    \"o\",\n",
    "    color=\"tab:blue\",\n",
    "    ms=5,\n",
    "    label=\"Best (for the marginal lik.)\",\n",
    ")\n",
    "ax.plot(\n",
    "    params[\"lengthscale\"],\n",
    "    params[\"variance\"],\n",
    "    \"o\",\n",
    "    color=\"tab:red\",\n",
    "    ms=5,\n",
    "    label=\"Initial guess\",\n",
    ")\n",
    "ax.set_title(\"(Log-)Marginal Likelihood\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (+) Alternative Kernel Functions\n",
    "So far we have focused exclusively on the RBF kernel.\n",
    "However, the choice of kernel function (along with its associated parameters) can have a significant effect on the overall Gaussian process model.\n",
    "Choosing the best kernel to fit your data is no simple task, and is a pertinent problem in many applied domains.<br>\n",
    "\n",
    "A brief discussion on this problem may be found here: <a target=\"_blank\" href=\"https://www.cs.toronto.edu/~duvenaud/cookbook/\">Kernel Cookbook</a>. \n",
    "\n",
    "Familiarise yourself better with these issues by implementing one or two additional kernels. \n",
    "\n",
    "For example, another popular kernel used in GP literature is the **Matérn kernel**.\n",
    "The Matérn covariance between two points separated by $d=||\\mathbf{x}-\\mathbf{z}||_2^2$ distance units is given by [Rasmussen & Williams](http://www.gaussianprocess.org/gpml/chapters/RW4.pdf) (Ch. 4).\n",
    "\n",
    "$$\n",
    "C_\\nu(d) = \\sigma^2\\frac{2^{1-\\nu}}{\\Gamma(\\nu)}\\Bigg(\\sqrt{2\\nu}\\frac{d}{\\rho}\\Bigg)^\\nu K_\\nu\\Bigg(\\sqrt{2\\nu}\\frac{d}{\\rho}\\Bigg),\n",
    "$$\n",
    "\n",
    "where $\\Gamma$ is the Gamma function, $K_\\nu$ is the modified Bessel function of the second kind, and $\\rho$ and $\\nu$ are non-negative parameters of the covariance.\n",
    "\n",
    "In practice:\n",
    "\n",
    "- for $\\nu = 1/2$:    $\\qquad C_{1/2}(d) = \\sigma^2\\exp\\left(-\\frac{d}{\\rho}\\right)$,\n",
    "\n",
    "- for $\\nu = 3/2$:    $\\qquad C_{3/2}(d) = \\sigma^2\\left(1+\\frac{\\sqrt{3}d}{\\rho}\\right)\\exp\\left(-\\frac{\\sqrt{3}d}{\\rho}\\right)$,\n",
    "\n",
    "- for $\\nu = 5/2$:    $\\qquad C_{5/2}(d) = \\sigma^2\\left(1+\\frac{\\sqrt{5}d}{\\rho}+\\frac{5d^2}{3\\rho^2}\\right)\\exp\\left(-\\frac{\\sqrt{5}d}{\\rho}\\right)$.\n",
    "\n",
    "**Exercise:**\n",
    "Pick one/two versions of the Matérn kernel and implement them. Plot the prior from the GP with this new kernel. Run the inference task and report/plot the results. *Hint:* You can reuse the code for the RBF kernel to compute the distances and you don't need to change the inference function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Parameter Optimisation using Gradients\n",
    "\n",
    "Optimise the hyperparameters of the model by maximizing the marginal likelihood of the model. \n",
    "To do so, you compute the gradients of the objective function with respect to the parameters being optimised.\n",
    "The general formula is given below:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\boldsymbol\\theta}\\mathcal{L}(\\boldsymbol\\theta) = - \\frac{1}{2} \\text{Tr} \\left(\\boldsymbol K^{-1} (\\nabla_{\\boldsymbol\\theta}\\kappa)(\\boldsymbol X, \\boldsymbol X) \\right) + \\frac{1}{2} \\boldsymbol{y}^{T}\\boldsymbol K^{-1} (\\nabla_{\\boldsymbol\\theta}\\kappa)(\\boldsymbol X, \\boldsymbol X) \\boldsymbol K^{-1} \\boldsymbol{y}.\n",
    "$$\n",
    "\n",
    "To give a more concrete example, the $(\\nabla_{\\boldsymbol\\theta}\\kappa)(\\boldsymbol X, \\boldsymbol X)$ term for the lengthscale parameter in the RBF kernel is computed as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\kappa}{\\partial l}(\\boldsymbol x, \\boldsymbol x')  = \\sigma_{f}^2 \\exp \\left( -\\dfrac {(\\boldsymbol{x}-\\boldsymbol{x}')^2}{2l^2} \\right)\\left( \\dfrac {(\\boldsymbol{x}-\\boldsymbol{x}')^2}{l^3} \\right)\n",
    "$$\n",
    "\n",
    "Then, you can use the classic gradient **ascent** algorithm,\n",
    "\n",
    "$$\n",
    "\\boldsymbol\\theta_{t+1} = \\boldsymbol\\theta_{t} + \\eta (\\nabla_{\\boldsymbol\\theta}\\mathcal{L})(\\boldsymbol\\theta_{t}) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The \"Millenial\" version (with the automatic differentiation of Jax)\n",
    "\n",
    "We don't really need to derive the gradients of the marginal likelihood w.r.t. parameters by hand. We can leverage the automatic differentiation engine in JAX! If you followed the instruction at the beginning of the notebook, all the operations we used are all differentiable. Let's take advantage of that! \n",
    "\n",
    "JAX has a pretty general automatic differentiation system.\n",
    "Generally, you can differentiate a (scalar) function with `jax.grad`. which operates on functions and returns a function. \n",
    "If you have a Python function `f` that evaluates the mathematical function $f$, then `jax.grad(f)` is a Python function that evaluates the mathematical function $\\nabla f$. \n",
    "That means `jax.grad(f)(x_i)` represents the value $\\nabla f(x_i)$.\n",
    "\n",
    "Another convenient function is `jax.value_and_grad` for efficiently computing both a function's value as well as its gradient's value:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally gradients are computed w.r.t. the first argument of the function, however differentiating with respect to standard Python containers just works, so use tuples, lists, and dicts (and arbitrary nesting) however you like.\n",
    "This is why in the beginning we put all the kernel parameters in a dictionary. \n",
    "\n",
    "This `grad` API has a direct correspondence to the excellent notation in Spivak's classic *Calculus on Manifolds* (1965), also used in Sussman and Wisdom's [*Structure and Interpretation of Classical Mechanics*](http://mitpress.mit.edu/sites/default/files/titles/content/sicm_edition_2/book.html) (2015) and their [*Functional Differential Geometry*](https://mitpress.mit.edu/books/functional-differential-geometry) (2013). Both books are open-access.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement this for our case. \n",
    "First of all, the next function implements the gradient update for the gradient ascent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_update(params, gradients, learning_rate):\n",
    "    updated_params = jax.tree_map(lambda p, g: p + learning_rate * g, params, gradients)\n",
    "    return updated_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Compute the gradient function of the marginal likelihood, using `jax.grad` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_marginal_likelihood =  # @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Complete the training loop, and also keep track of the marginal during optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"lengthscale\": 1.0, \"variance\": 1.0}  # Initial guess\n",
    "marginals = []  # Keep track of the marginal for plotting here\n",
    "for _ in range(2000):\n",
    "    value =  # @@ COMPLETE @@ #\n",
    "    gradients =  # @@ COMPLETE @@ #\n",
    "    params =  # @@ COMPLETE @@ #\n",
    "    marginals.append(value)\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Plot the marginal likelihood during optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[7, 3])\n",
    "# @@ COMPLETE @@ #\n",
    "\n",
    "ax.set_title(\"Optimization of the marginal likelihood\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Marginal Likelihood\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Finally, with these parameters, plot the predictive posterior as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pro tip:** Note that the RBF kernel parameters $l$ and $\\sigma_\\mathrm{f}^2$ are always expected to be positive. It is possible that the optimisation algorithm attempts to evaluate the marginal likelihood in regions of the parameter space where one or more of these parameters are negative, leading to numerical issues. \n",
    "A commonly-used technique to enforce this condition is to work with a transformed version of covariance parameters using the logarithm transformation. In particular, define $\\psi_l = \\log(l)$ and $\\psi_{f} = \\log(\\sigma_{f}^2 )$ and optimise with respect to the $\\Psi$ parameters. \n",
    "The optimisation problem in the transformed space is now unbounded, and the gradient of the likelihood should be computed with respect to the $\\Psi$ parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
