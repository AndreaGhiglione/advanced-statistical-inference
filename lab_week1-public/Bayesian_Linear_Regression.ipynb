{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Advanced Statistical Inference -- Bayesian Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In this session, you'll start to implement some basic Bayesian models, \n",
    "starting from the simple Bayesian linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import warnings\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.font_manager\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "import scipy.stats\n",
    "from matplotlib import rc\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "preamble = r\"\"\"\\renewcommand{\\familydefault}{\\sfdefault}\n",
    "            \\usepackage{sansmath} \\sansmath  \\usepackage{amsmath}\"\"\"\n",
    "rc(\"font\", **{\"family\": \"sans-serif\", \"sans-serif\": \"DejaVu Sans\"})\n",
    "rc(\"text\", **{\"usetex\": False, \"latex.preamble\": preamble})\n",
    "rc(\"figure\", **{\"dpi\": 200})\n",
    "rc(\n",
    "    \"axes\",\n",
    "    **{\"spines.right\": False, \"spines.top\": False, \"xmargin\": 0, \"ymargin\": 0.05}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1. A regression dataset\n",
    "Create a simple 1D regression dataset using the `make_regression(...)` function and plot it.\n",
    "For the moment, keep the noise variance $\\sigma_\\mathrm{n}$ small.\n",
    "NB. For better reproducibility, please remember to fix the Numpy's random seed. \n",
    "For Jupyter notebooks, this needs to be done at the beginning of all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "true_function = lambda x: 0.5 * ((x - 1) ** 2) - 3\n",
    "\n",
    "\n",
    "def make_regression(n, sn2=0.1):\n",
    "    x = np.random.uniform(-3, 3, n)\n",
    "    y = true_function(x) + np.sqrt(sn2) * np.random.randn(*x.shape)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "sn2 = 1.5\n",
    "x, y = make_regression(30, sn2=sn2)\n",
    "xp = np.linspace(-4, 4)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 3], dpi=250)\n",
    "ax.plot(xp, true_function(xp), \"--k\", zorder=0, label=\"True function\")\n",
    "ax.scatter(x, y, edgecolor=\"black\", linewidth=1, facecolor=\"xkcd:orange\")\n",
    "ax.set_title(\"A regression dataset\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.margins(0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. A review on Gaussian likelihood\n",
    "Let's start from the basics. Remember what a likelihood is.\n",
    "The **likelihood** measures the goodness of fit of a statistical model to samples of data for given values of \n",
    "the unknown model parameters.\n",
    "It's computed from the joint probability distribution, but viewed and used as **function** \n",
    "of the parameters only, thus treating the random variables as fixed at the observed values.\n",
    "\n",
    "A Gaussian likelihood is defined as \n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mathbf{y}|\\mathbf{w}, \\mathbf{X}, \\sigma_\\mathrm{n}) = \\prod_{i=1}^N p(y_i|\\mathbf{w}, {\\mathbf{x}}_i, \\sigma_\\mathrm{n}) = \\prod_{i=1}^N \\mathcal{N}(y_i|\\tilde y_i, \\sigma_\\mathrm{n})\n",
    "\\end{equation}\n",
    "\n",
    "where, for linear regression, $\\tilde y_i = \\mathbf{w}^\\top {\\mathbf{x}}_i$.\n",
    "For numerical stability, instead of using the vanilla likelihood, we will use the **log-likelihood**.\n",
    "\n",
    "\\begin{equation}\n",
    "\\log p(\\mathbf{y}|\\mathbf{w}, \\mathbf{X}, \\sigma_\\mathrm{n}) =  \\sum_{i=1}^N  \\log\\mathcal{N}(y_i|\\tilde y_i, \\sigma_\\mathrm{n})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Write a function to compute the log-density of a normal distribution at position $x$, given $\\mu$ and $\\sigma^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def lognormal(x, mu, var):\n",
    "    # @@ COMPLETE @@ #\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "For the moment, assume that for the sample $i^{\\mathrm{th}}$, you predict $\\tilde y = 0.3$ and $\\sigma_\\mathrm{n} = 1$. \n",
    "You know that $y = 0.4$. \n",
    "Complete the following function `gaussian_loglik(...)`, then compute the (log)likelihood for this sample and show its position on the Gaussian density with a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gaussian_loglik(y, y_tilde, sn2):\n",
    "    # @@ COMPLETE @@ #\n",
    "    return\n",
    "\n",
    "\n",
    "def plot_gaussian(mu, var, plot_log=False, **kwargs):\n",
    "    \"\"\"A simple util to plot a gaussian pdf\"\"\"\n",
    "    x = np.linspace(mu - 5 * np.sqrt(var), mu + 5 * np.sqrt(var), 200)\n",
    "    y = lognormal(x, mu, var) if plot_log else np.exp(lognormal(x, mu, var))\n",
    "    ax = kwargs.pop(\"ax\", plt.gca())\n",
    "    ax.plot(x, y, **kwargs)\n",
    "    return ax\n",
    "\n",
    "\n",
    "y_obs =   # @@ COMPLETE @@ #\n",
    "y_tilde =   # @@ COMPLETE @@ #\n",
    "sn2 =   # @@ COMPLETE @@ #\n",
    "sample_ll =  # @@ COMPLETE @@ #\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 3])\n",
    "plot_gaussian(y_tilde, sn2, ax=ax, label=r\"$\\mathcal{N}_y(\\widetilde{y}, \\sigma_n^2)$\")\n",
    "ax.vlines(y_obs, 0, np.exp(sample_ll), color=\"k\")\n",
    "ax.vlines(\n",
    "    y_tilde, 0, np.exp(gaussian_loglik(y_tilde, y_tilde, sn2)), ls=\"--\", color=\"k\"\n",
    ")\n",
    "ax.plot(y_obs, np.exp(sample_ll), \"ok\", label=r\"Likelihood\")\n",
    "ax.text(y_tilde + 0.1, 0.02, r\"$\\widetilde{y}$\")\n",
    "ax.text(y_obs - 0.2, 0.02, r\"$y$\")\n",
    "ax.set_ylim(0)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bayesian Linear regression\n",
    "\n",
    "In this section, you'll start to implement the Bayesian linear regression model.\n",
    "Let's start by creating the **design matrix** $\\mathbf{X}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\left[ {\\begin{array}{ccccc}\n",
    "   1 & x_1^1 & \\dots & x_1^K\\\\\n",
    "   1 & x_2^1 & \\dots & x_2^K\\\\\n",
    "   \\vdots &    \\vdots & &   \\vdots \\\\\n",
    "   1 & x_N^1 & \\dots & x_N^K\\\\\n",
    "  \\end{array} } \\right]\n",
    "$$\n",
    "\n",
    "**Exercise:**\n",
    "Complete the following function `build_features(...)` to build $\\mathbf{X}$.\n",
    "This can be done in many ways. One of them is using a double list comprehension (one index for the row and one for the column), while another one is using the numpy `column_stack()` function (highly suggested). In any case, inspect $\\mathbf{X}$ to make sure it looks OK (show the first entries). To fit higher order polynomials, we need to add extra columns to $\\mathbf{X}$, therefore build it with $K$ as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def build_features(x, K):\n",
    "    # @@ COMPLETE @@ #\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_features(x, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the lecture notes, let's define the prior on the parameters $\\mathbf{w}$ as \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{S}) \n",
    "\\end{equation}\n",
    "\n",
    "For sake of simplicity, assume the covariance matrix $\\mathbf{S}$ to be diagonal $\\mathbf{S} = \\sigma_\\mathrm{w}^2\\mathbf{I}$.\n",
    "Remember that the likelihood is defined as $p(\\mathbf{y}|\\mathbf{w}, \\mathbf{X}, \\sigma_\\mathrm{n}) = \\mathcal{N}(\\mathbf{y}|\\mathbf{X}\\mathbf{w}, \\sigma_\\mathrm{n}^2\\mathbf{I})$.\n",
    "In this case, the posterior is analitic and follows this form:\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mathbf{w}|\\mathbf{X}, \\mathbf{y}, \\sigma_\\mathrm{n}) = \\mathcal{N}\\left(\\frac{1}{\\sigma^2_\\mathrm{n}}\\mathbf{\\Sigma}\\mathbf{X}^\\top\\mathbf{y}, \\mathbf{\\Sigma} \\right)\n",
    "\\end{equation}\n",
    "where $\\mathbf{\\Sigma}^{-1} = \\left(\\frac{1}{\\sigma^2_\\mathrm{n}}\\mathbf{X}^\\top\\mathbf{X} + \\mathbf{S}^{-1}  \\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the dimensionality of $\\mathbf{\\Sigma}$? How much does it cost to compute that inverse? Do you know which algorithm you should use to have numerically stable results? Remember that computing $\\mathbf{A}^{-1}\\mathbf{z}$ means in practice solving a linear system.\n",
    "\n",
    "**Exercise:** Complete the following function to compute the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_posterior(X, y, sw2, sn2):\n",
    "    # @@ COMPLETE @@ #\n",
    "    return w_posterior_mean, w_posterior_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execise:** Compute the posterior for the regression dataset. For the moment, place $\\sigma_\\mathrm{w}^2=1$ and start with polynomial of order 1. Finally, print the posterior mean and covariance. Comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "sw2 = 1\n",
    "K = 1\n",
    "\n",
    "X =   # @@ COMPLETE @@ #\n",
    "\n",
    "w_posterior_mean, w_posterior_cov = # @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(w_posterior_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(w_posterior_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "x_ = np.linspace(-3, 3, 50)\n",
    "y_ = np.linspace(-3, 3, 50)\n",
    "X_, Y_ = np.meshgrid(x_, y_)\n",
    "pos = np.empty(X_.shape + (2,))\n",
    "pos[:, :, 0] = X_\n",
    "pos[:, :, 1] = Y_\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "ax = fig.add_subplot(1, 2, 1, projection=\"3d\")\n",
    "\n",
    "rv = scipy.stats.multivariate_normal(np.zeros(2), sw2 * np.eye(2))\n",
    "plot_config = dict(\n",
    "    cmap=\"viridis\", linewidth=0, antialiased=False, ccount=500, rcount=500\n",
    ")\n",
    "ax.plot_surface(X_, Y_, rv.pdf(pos), **plot_config)\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_zlabel(\"\")\n",
    "ax.set_title(\"Prior\")\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2, projection=\"3d\")\n",
    "rv = scipy.stats.multivariate_normal(w_posterior_mean, w_posterior_cov)\n",
    "ax.plot_surface(X_, Y_, rv.pdf(pos), **plot_config)\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_zlabel(\"\")\n",
    "\n",
    "ax.set_title(\"Posterior\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predictions\n",
    "Now it's time to make predictions. \n",
    "All our motivation for being Bayesian was to be able to average predictions at $\\mathbf{x}_\\mathrm{new}$, for all possible $\\mathbf{w}$.\n",
    "This is possible by computing the following expectation:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{E}_{p(\\mathbf{w}|\\mathbf{X}, \\mathbf{y}, \\sigma_\\mathrm{n})}\\mathcal{N}(\\mathbf{w}^\\top\\mathbf{x}_\\mathrm{new}, \\sigma^2_\\mathrm{n}) = \\int \\mathcal{N}(\\mathbf{w}^\\top\\mathbf{x}_\\mathrm{new}, \\sigma^2_\\mathrm{n}) p(\\mathbf{w}|\\mathbf{X}, \\mathbf{y}, \\sigma_\\mathrm{n}) \\mathrm{d}\\mathbf{w}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Prove that \n",
    "$\\mathbf{E}_{p(\\mathbf{w}|\\mathbf{X}, \\mathbf{y}, \\sigma_\\mathrm{n})}\\mathcal{N}(\\mathbf{w}^\\top\\mathbf{x}_\\mathrm{new}, \\sigma^2_\\mathrm{n}) = \n",
    "\\mathcal{N}(\\mathbf{x}_\\mathrm{new}^\\top\\mathbf{\\mu}, \\sigma^2_\\mathrm{n} + \\mathbf{x}_\\mathrm{new}^\\top\\mathbf{\\Sigma}\\mathbf{x}_\\mathrm{new})$, where $\\mathbf{\\mu}$ and $\\mathbf{\\Sigma}$ are the posterior mean and covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "Write a function to compute the predictive distribution. \n",
    "Usually we want to do this for multiple points, hence the for-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_predictive(Xt, w_mean, w_cov, sn2):\n",
    "    def _compute_predictive_single_point(xt_i, w_mean, w_cov, sn2):\n",
    "        yt_i_mean = # @@ COMPLETE @@ #\n",
    "        yt_i_var = # @@ COMPLETE @@ #\n",
    "        return yt_i_mean, yt_i_var\n",
    "\n",
    "    yt_mean, yt_var = np.zeros(len(Xt)), np.zeros(len(Xt))\n",
    "    for i, xt_i in enumerate(Xt):  # Loop on all the points\n",
    "        yt_mean[i], yt_var[i] = _compute_predictive_single_point(\n",
    "            xt_i, w_mean, w_cov, sn2\n",
    "        )\n",
    "\n",
    "    return yt_mean, yt_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Compute and plot the predictive distribution for 100 points between -4 and +4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "xt = np.linspace(-4, 4, 250)\n",
    "Xt = build_features(xt, K)\n",
    "y_mean, y_var =   # @@ COMPLETE @@ #\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 3])\n",
    "ax.scatter(x, y, edgecolor=\"black\", linewidth=1, facecolor=\"xkcd:orange\", zorder=10)\n",
    "ax.plot(xt, y_mean, color=\"tab:blue\", lw=3)\n",
    "\n",
    "lb = y_mean - 2 * np.sqrt(y_var)\n",
    "ub = y_mean + 2 * np.sqrt(y_var)\n",
    "ax.fill_between(xt, lb, ub, color=\".80\", lw=0)\n",
    "ax.set_title(f\"Predictive distribution with order {K}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** You can also sample from the posterior and compute the function values. A simple way to do so is to sample from the posterior on the weights and then from the noise model (in our case, its $\\mathcal{N}(0, \\sigma_n^2)$.\n",
    "Complete and use the utility function below: sample 30 times the predictive posterior and plot it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def sample(Xt, w_mean, w_cov, sn2, N):\n",
    "    def _sample_single(Xt, w_mean, w_cov, sn2):\n",
    "        # @@ COMPLETE @@ #\n",
    "        return y_sample\n",
    "\n",
    "    samples = np.zeros((N, len(Xt)))\n",
    "    for i in range(N):\n",
    "        samples[i] = _sample_single(Xt, w_mean, w_cov, sn2)\n",
    "    return samples\n",
    "\n",
    "\n",
    "samples = sample(Xt, w_posterior_mean, w_posterior_cov, sn2, N=30)\n",
    "fig, ax = plt.subplots(figsize=[5, 3])\n",
    "ax.scatter(x, y, edgecolor=\"black\", linewidth=1, facecolor=\"xkcd:orange\", zorder=10)\n",
    "ax.plot(xt, samples.T, \"tab:blue\", alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Try now with different polynomial order. Let's say 2, 5, 10, 15. Compute the design matrix, the posterior on $\\mathbf{w}$ and the predictive posterior. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def bayesian_linear_regression(x, y, xt, K, sn2, sw2):\n",
    "    X = build_features(x, K) \n",
    "    Xt = build_features(xt, K)  \n",
    "    w_posterior_mean, w_posterior_cov =  # @@ COMPLETE @@ #\n",
    "    y_mean, y_var =  # @@ COMPLETE @@ #\n",
    "    samples =   # @@ COMPLETE @@ #\n",
    "    return y_mean, y_var, samples\n",
    "\n",
    "\n",
    "poly_orders = [2, 5, 10, 12]\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=[10, 6], sharex=True, sharey=True)\n",
    "axs = axs.reshape(-1)\n",
    "for ax, K in zip(axs, poly_orders):\n",
    "    y_mean, y_var, samples = bayesian_linear_regression(x, y, xt, K, sn2, sw2)\n",
    "\n",
    "    lb = y_mean - 2 * np.sqrt(y_var)\n",
    "    ub = y_mean + 2 * np.sqrt(y_var)\n",
    "\n",
    "    ax.fill_between(xt, lb, ub, color=\".80\", lw=0)\n",
    "    ax.plot(xt, samples.T, \"tab:blue\", alpha=0.2)\n",
    "    ax.plot(xt, y_mean, color=\"tab:blue\", lw=3)\n",
    "    ax.scatter(x, y, edgecolor=\"black\", linewidth=1, facecolor=\"xkcd:orange\", zorder=10)\n",
    "    ax.set_ylim(-10, 20)\n",
    "    ax.set_title(\"Order %d\" % K)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "Play with the next cell and try to answer the following questions:\n",
    "1. Set n=0; what is the plot showing?\n",
    "2. Try to increase the number of points \"observed\"; what is happening to the posterior?\n",
    "3. Try to increase the polynomial order; how are the functions behaving?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import fixed, interact, interact_manual, interactive\n",
    "\n",
    "\n",
    "def animate(n, K):\n",
    "    y_mean, y_var, samples = bayesian_linear_regression(x[:n], y[:n], xt, K, sn2, sw2)\n",
    "    fig, ax = plt.subplots(figsize=[5, 3])\n",
    "    ax.scatter(\n",
    "        x[:n], y[:n], edgecolor=\"black\", linewidth=1, facecolor=\"xkcd:orange\", zorder=10\n",
    "    )\n",
    "    ax.plot(xt, samples.T, \"tab:blue\", alpha=0.3)\n",
    "    ax.set_ylim(-10, 20)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "interaction = interact(\n",
    "    animate,\n",
    "    n=widgets.IntSlider(min=0, max=len(x), step=1, value=0, continuous_update=False),\n",
    "    K=widgets.IntSlider(min=0, max=10, step=1, value=1, continuous_update=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate you model: the marginal likelihood\n",
    "\n",
    "There are several ways in which you can compute the goodness of your model. The first is the likelihood itself.\n",
    "\n",
    "**Question:** Compute the loglikelihood for model with order from 0 to 7 and plot it. Comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_likelihood(x, y, K, sn2, sw2):\n",
    "    y_mean, y_var, _ = bayesian_linear_regression(x, y, x, K, sn2, sw2)\n",
    "    # @@ COMPLETE @@ #\n",
    "    return\n",
    "\n",
    "\n",
    "Ks = np.arange(1, 8)\n",
    "mll = [evaluate_likelihood(x, y, K, sn2, sw2) for K in Ks]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 3])\n",
    "ax.plot(Ks, mll, \"-ok\")\n",
    "ax.margins(0.05)\n",
    "ax.set_title(\"Log-Likelihood\")\n",
    "ax.set_xlabel(\"Model (e.g. polyn. order)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Try to answer: How likely is $\\mathbf{y}$ given $\\mathbf{X}$ *and* the model (‘first/second/... order polynomial’)? Is it the same likelihood as before?\n",
    "\n",
    "So far, we’ve ignored $p(\\mathbf{y}|\\mathbf{X}, \\sigma^2_\\mathrm{n})$, the normalising thing in Bayes rule. Being a normalization constant, it has to be equal to \n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mathbf{y}|\\mathbf{X}, \\sigma^2_\\mathrm{n}) = \\int p(\\mathbf{y}|\\mathbf{X}, \\mathbf{w}, \\sigma^2_\\mathrm{n})\n",
    "p(\\mathbf{w})\\mathrm{d}\\mathbf{w}\n",
    "\\end{equation}\n",
    "\n",
    "We’re averaging over all values of $\\mathbf{w}$ to get a value for how good the model is.\n",
    "\n",
    "**Question:** Suppose the prior being $\\mathcal{N}(\\mu_0, \\mathbf{\\Sigma}_0)$ and the likelihood $\\mathcal{N}(\\mathbf{X}\\mathbf{w}, \\sigma^2_\\mathrm{n} \\mathbf{I})$. Derive the marginal likelihood (hint: don't solve the integral -- check the rules for Gaussian conditioning and marginalization) (big hint: check the lecture notes).\n",
    "\n",
    "**Exercise:** Write a function to compute the marginal likelihood. Remember: this is a *likelihood* not a density. You should return a number not a density. For simplicity, assume $\\mu_0 = 0$ and $\\Sigma_0 = \\sigma^2_\\mathrm{w}\\mathbf{I}$. Use `scipy.stats.multivariate_normal` for computing the logpdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "def marginal_likelihood(X, y, sw2, sn2):\n",
    "    return  # @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Do the sample plot as before, but now plot the marginal likelihood. You should see a clear pattern here; comment the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_marginal_likelihood(x, y, K, sn2, sw2):\n",
    "    X = build_features(x, K)\n",
    "    return  # @@ COMPLETE @@ #\n",
    "\n",
    "\n",
    "poly_orders = range(1, 8)\n",
    "mll = [evaluate_marginal_likelihood(x, y, K, sn2, sw2) for K in poly_orders]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 3])\n",
    "ax.plot(poly_orders, mll, \"-ok\")\n",
    "ax.margins(0.05)\n",
    "\n",
    "ax.set_title(\"Log-Marginal Likelihood\")\n",
    "ax.set_xlabel(\"Model (e.g. polyn. order)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. When polynomial features are not enough (++)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "\n",
    "def snelson1d(path):\n",
    "    filename = os.path.join(os.path.expanduser(path), \"snelson1d.mat\")\n",
    "    url = \"https://github.com/markvdw/gp_upper/raw/master/notebooks/snelson1d.mat\"\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"Downloading from\", url)\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "    data = loadmat(filename)\n",
    "    idx = np.random.RandomState(0).permutation(len(data[\"X\"]))[:75]\n",
    "\n",
    "    return data[\"X\"][idx, 0] - 3, data[\"Y\"][idx, 0]\n",
    "\n",
    "\n",
    "x, y = snelson1d(\"/tmp/\")\n",
    "xt = np.linspace(-6, 6, 200)\n",
    "sn2 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[6, 4])\n",
    "ax.scatter(x, y, edgecolor=\"black\", linewidth=1, facecolor=\"xkcd:orange\", zorder=10)\n",
    "ax.set_title(\"Snelson dataset\")\n",
    "ax.set_xlim(-4, 4)\n",
    "ax.set_ylim(-3, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just from looking at the dataset, you can imagine that fitting this dataset will be hard. And it is (with polynomial features).\n",
    "Maybe it becomes easier with different type of features.\n",
    "Remember that you can choose whatever you want to build the design matrix. Take a look at the following \n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\sqrt{\\frac{\\alpha^2}{K}}\\left[ \n",
    "    {\\begin{array}{cccccc}\n",
    "         \\cos(\\omega_1x_1) &  \\sin(\\omega_1x_1) & \\dots &   \\cos(\\omega_Kx_1) & \\sin(\\omega_Kx_1) \\\\\n",
    "         \\cos(\\omega_1x_2) &  \\sin(\\omega_1x_2) & \\dots &   \\cos(\\omega_Kx_2) & \\sin(\\omega_Kx_2) \\\\\n",
    "         \\vdots &  \\vdots &       &   \\vdots  \\\\\n",
    "          \\cos(\\omega_1x_N) &  \\sin(\\omega_1x_N) & \\dots &   \\cos(\\omega_Kx_N) & \\sin(\\omega_Kx_N) \\\\\n",
    "  \\end{array} } \n",
    "\\right]\n",
    "\\quad\n",
    "\\mathrm{where}\n",
    "\\quad \n",
    "\\omega_i \\sim \\mathcal N (0, \\lambda)\n",
    "$$\n",
    "\n",
    "Now, the next formulation of the design matrix $\\mathbf{X}$ might seems to come completely out of the blue, but it's not (for those of you interested, this is the random feature expansion of the RBF kernel -- join the next lecture to know more).\n",
    "\n",
    "For simplicity below you have the code to compute $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Now run Bayesian linear regression on this dataset with this new set of features (NB. You might need many MANY features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def build_fourier_features(X: np.ndarray, K: int):\n",
    "    omega = np.random.RandomState(0).normal(size=K) / 0.5\n",
    "    sinX_ = np.column_stack([np.sin(X * omega[k]) for k in range(K)])\n",
    "    cosX_ = np.column_stack([np.cos(X * omega[k]) for k in range(K)])\n",
    "    return np.concatenate([sinX_, cosX_], -1) * np.sqrt(1 / K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_linear_regression(x, y, xt, K, sn2, sw2):\n",
    "    # @@ COMPLETE @@ #\n",
    "    return y_mean, y_var, samples\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "y_mean, y_var, samples = bayesian_linear_regression(x, y, xt, 100, sn2, sw2)\n",
    "\n",
    "lb = y_mean - 2 * np.sqrt(y_var)\n",
    "ub = y_mean + 2 * np.sqrt(y_var)\n",
    "\n",
    "ax.fill_between(xt, lb, ub, color=\".80\", lw=0)\n",
    "ax.plot(xt, samples.T, \"tab:blue\", alpha=0.2)\n",
    "ax.plot(xt, y_mean, color=\"tab:blue\", lw=3)\n",
    "ax.scatter(x, y, edgecolor=\"black\", linewidth=1, facecolor=\"xkcd:orange\", zorder=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import fixed, interact, interact_manual, interactive\n",
    "\n",
    "\n",
    "def animate_random_features(K):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    y_mean, y_var, samples = bayesian_linear_regression(x, y, xt, K, sn2, sw2)\n",
    "\n",
    "    lb = y_mean - 2 * np.sqrt(y_var)\n",
    "    ub = y_mean + 2 * np.sqrt(y_var)\n",
    "\n",
    "    ax.fill_between(xt, lb, ub, color=\".80\", lw=0)\n",
    "    ax.plot(xt, samples.T, \"tab:blue\", alpha=0.2)\n",
    "    ax.plot(xt, y_mean, color=\"tab:blue\", lw=3)\n",
    "    ax.scatter(x, y, edgecolor=\"black\", linewidth=1, facecolor=\"xkcd:orange\", zorder=10)\n",
    "    ax.set_ylim(-3, 3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "interaction = interact(\n",
    "    animate_random_features,\n",
    "    K=widgets.IntSlider(min=1, max=500, step=10, value=10, continuous_update=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. A more complex model: being Bayesian on the noise (++)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready spice things up! Everything we did, we did it assuming that the noise variance $\\sigma_\\mathrm{n}^2$ was known. This is a far too restrictive assumption in practice. \n",
    "There are several ways to choose $\\sigma_\\mathrm{n}^2$ (cross-validation, maximization of the marginal likelihood w.r.t. $\\sigma_\\mathrm{n}^2$, ...).\n",
    "But this -- being a course on Bayesian inference -- requires to develop a solution in a \"Bayesian\" way: place a prior on $\\sigma_\\mathrm{n}^2$ and infer a posterior given some data.\n",
    "\n",
    "As usual, the likelihood has the form $p(\\mathbf{y}|\\mathbf{w}, \\mathbf{X}, \\sigma_\\mathrm{n}) = \\mathcal{N}(\\mathbf{X}\\mathbf{w}, \\sigma^2_\\mathrm{n} \\mathbf{I})$.\n",
    "\n",
    "One can show that the natural conjugate prior is a normal-inverse Gaussian distribution with the following form:\n",
    "\n",
    "\\begin{align}\n",
    "p(\\mathbf{w}, \\sigma^2_\\mathrm{n})  &= \\mathrm{NIG}(\\mathbf{w}, \\sigma^2_\\mathrm{n}|\\mu_0, \\mathbf{\\Sigma}_0, a_0, b_0) = \\\\\n",
    "                                    &= \\mathcal{N}(\\mathbf{w}|\\mu_0,\\sigma^2_\\mathrm{n}\\mathbf{\\Sigma}_0)\\mathrm{IG}(\\sigma^2_\\mathrm{n}|a_0, b_0)\n",
    "\\end{align}\n",
    "\n",
    "where IG is the inverse Gamma distribution.\n",
    "\n",
    "**Exercise:** Since changes are you never saw an inverse Gamma distribution, use the following cell to play with it. Try to change the parameters $a_0$ and $b_0$. What happens if you both set them to 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "a0, b0 = 1, 3\n",
    "rv = scipy.stats.invgamma(a=a0, scale=b0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[4, 2.5])\n",
    "xplot = np.linspace(0, 10, 100)\n",
    "ax.plot(xplot, rv.pdf(xplot))\n",
    "ax.set_title(\"IG(%.1f,%.1f)\" % (a0, b0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can derive the posterior $p(\\mathbf{w},\\sigma_\\mathrm{n}^2 | \\mathbf{X}, \\mathbf{y})$ as follows\n",
    "\n",
    "\\begin{align}\n",
    "p(\\mathbf{w},\\sigma_\\mathrm{n}^2 | \\mathbf{X}, \\mathbf{y}) &= \\mathrm{NIG}(\\mathbf{w},\\sigma_\\mathrm{n}^2|\\mu, \\mathbf{\\Sigma}, a, b)\\\\\n",
    "\\mathbf{\\Sigma} &= (\\mathbf{\\Sigma}_0^{-1} + \\mathbf{X}^\\top\\mathbf{X})^{-1}\\\\\n",
    "\\mu &= \\mathbf{\\Sigma}(\\mathbf{\\Sigma}_0^{-1}\\mu_0 + \\mathbf{X}^\\top\\mathbf{y}) \\\\\n",
    "a &= a_0 + n/2 \\\\ \n",
    "b &= b_0 + \\frac{1}{2}(\\mu_0^\\top\\mathbf{\\Sigma}_0\\mu_0 + \\mathbf{y}^\\top\\mathbf{y} - \\mu^\\top\\mathbf{\\Sigma}\\mu )\n",
    "\\end{align}\n",
    "\n",
    "This is left as an exercise to the reader (joking aside if you want to know more check Sec. 7.6.3.1 of \"Machine Learning: A probabilistic perspective\" by K.P. Murphy).\n",
    "From this formulation, you can derive the two marginals on $\\mathbf{w}$ and ${\\sigma_\\mathrm{n}^2}$, which are easier to understand.\n",
    "\n",
    "\\begin{align}\n",
    "p(\\sigma_\\mathrm{n}^2|\\mathrm{X}, \\mathrm{y}) &= \\mathrm{IG}(a,b)\\\\\n",
    "p(\\mathbf{w}|\\mathrm{X}, \\mathrm{y}) &= \\mathcal{T}\\left(\\mu, \\frac{b}{a}\\Sigma, 2a\\right)\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathcal{T}$ is a student-T distribution. \n",
    "\n",
    "Finally, the predictive distribution a new test points $\\mathbf{X}_{\\mathrm{new}}$ is again a student-T distribution with the following form, \n",
    "\n",
    "\\begin{align}\n",
    "p(\\mathbf{y}_\\mathrm{new}| \\mathbf{X}_\\mathrm{new}, \\mathbf{X}, \\mathbf{y}) = \\mathcal{T}\\left(\\mathbf{X}_\\mathrm{new}\\mu, \\frac{b}{a}(I + \\mathbf{X}_\\mathrm{new}\\mathbf{\\Sigma}\\mathbf{X}_\\mathrm{new}^\\top), 2a \\right)\n",
    "\\end{align}\n",
    "\n",
    "**Exercise:** Write the function to compute the posterior following the formula above. Compute also the predictive posterior and plot few samples from it. \n",
    "\n",
    "For the prior, choose $\\mu_0 = 0$, $\\mathbf{\\Sigma}_0 = \\mathbf{I}$ and $a_0 = b_0 = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_posterior(X, y, mu0, Sigma0, a0, b0):\n",
    "    return  # @@ COMPLETE @@ #\n",
    "\n",
    "\n",
    "def compute_predictive_posterior(X, mu, Sigma, a, b):\n",
    "    return  # @@ COMPLETE @@ #\n",
    "\n",
    "\n",
    "def sample_from_student_t(m, S, dof, n=1):\n",
    "    d = len(m)\n",
    "    if dof == np.inf:\n",
    "        x = 1.0\n",
    "    else:\n",
    "        x = np.random.chisquare(dof, n) / dof\n",
    "    z = np.random.multivariate_normal(np.zeros(d), S, (n,))\n",
    "    return m + z / np.sqrt(x)[:, None]\n",
    "\n",
    "\n",
    "def sample(Xt, w_mean, w_cov, w_dof, a, b, N):\n",
    "    def _sample_single(Xt, w_mean, w_cov, w_dof, a, b):\n",
    "        # @@ COMPLETE @@ #\n",
    "        return y_sample\n",
    "\n",
    "    samples = np.zeros((N, len(Xt)))\n",
    "    for i in range(N):\n",
    "        samples[i] = _sample_single(Xt, w_mean, w_cov, w_dof, a, b)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "sn2 = 1.5\n",
    "x, y = make_regression(30, sn2=sn2)\n",
    "xt = np.linspace(-4, 4, 150)\n",
    "\n",
    "# @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Compare this plot with the previous one with order 1. Do you see any difference? Comment the results.\n",
    "\n",
    "**Exercise:** Do the same a before, but now with order 2, 3, 5 and 10."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
